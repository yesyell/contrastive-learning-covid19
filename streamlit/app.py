import streamlit as st
import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
from scipy.io import wavfile
from io import BytesIO
from streamlit_webrtc import webrtc_streamer, AudioProcessorBase, WebRtcMode
import torch
import torch.nn as nn
import torchvision.models as models
from PIL import Image
import os
from audio import change_volume

# OpenMP ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# streamlit ì›¹ ë°°í¬ë¥¼ ìœ„í•œ ì ˆëŒ€ê²½ë¡œ í¬í•¨
def get_absolute_path(relative_path):
    current_dir = os.path.dirname(os.path.abspath(__file__))
    return os.path.join(current_dir, relative_path)

logo_path = get_absolute_path('forapp/logo.jpg')
logo = Image.open(logo_path)

class MoCoV2(nn.Module):
    def __init__(self, base_encoder, dim=128, K=4096, m=0.999, T=0.07):
        super(MoCoV2, self).__init__()
        self.K = K
        self.m = m
        self.T = T

        self.encoder_q = base_encoder()
        self.encoder_q.fc = nn.Sequential(
            nn.Linear(self.encoder_q.fc.in_features, 2048),
            nn.ReLU(),
            nn.Linear(2048, dim)
        )

        self.encoder_k = base_encoder()
        self.encoder_k.fc = nn.Sequential(
            nn.Linear(self.encoder_k.fc.in_features, 2048),
            nn.ReLU(),
            nn.Linear(2048, dim)
        )

        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):
            param_k.data.copy_(param_q.data)
            param_k.requires_grad = False

        self.register_buffer("queue", torch.randn(dim, K))
        self.queue = nn.functional.normalize(self.queue, dim=0)

        self.register_buffer("queue_ptr", torch.zeros(1, dtype=torch.long))
        self.register_buffer("queue_covid", torch.zeros(K, dtype=torch.long))

    @torch.no_grad()
    def _momentum_update_key_encoder(self):
        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):
            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)

    @torch.no_grad()
    def _dequeue_and_enqueue(self, keys, covid):
        batch_size = keys.shape[0]
        ptr = int(self.queue_ptr)

        if ptr + batch_size > self.K:
            batch_size = self.K - ptr
            keys = keys[:batch_size]
            covid = covid[:batch_size]

        self.queue[:, ptr:ptr + batch_size] = keys.T
        self.queue_covid[ptr:ptr + batch_size] = covid

        ptr = (ptr + batch_size) % self.K
        self.queue_ptr[0] = ptr

    def forward(self, im_q, im_k, covid):
        q = self.encoder_q(im_q)
        q = nn.functional.normalize(q, dim=1)

        with torch.no_grad():
            self._momentum_update_key_encoder()
            k = self.encoder_k(im_k)
            k = nn.functional.normalize(k, dim=1)

        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)

        queue = self.queue.clone().detach()
        queue_covid = self.queue_covid.clone().detach()

        neg_idx = (queue_covid != covid.unsqueeze(1)).float()
        l_neg = torch.einsum('nc,ck->nk', [q, queue])
        l_neg = l_neg * neg_idx

        logits = torch.cat([l_pos, l_neg], dim=1)
        logits /= self.T

        labels = torch.zeros(logits.shape[0], dtype=torch.long).cuda()

        self._dequeue_and_enqueue(k, covid)

        return logits, labels

class LinearClassifier(nn.Module):
    def __init__(self, feature_dim, num_classes):
        super(LinearClassifier, self).__init__()
        self.fc1 = nn.Linear(feature_dim + 1, 256)  # +1 for COVID_symptoms
        self.fc2 = nn.Linear(256, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x, covid):
        x = torch.cat([x, covid.unsqueeze(1)], dim=1)
        x = self.relu(self.fc1(x))
        return self.fc2(x)


# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
@st.cache_resource
def load_model():
    moco = MoCoV2(base_encoder=models.resnet50, K=4096)
    classifier = LinearClassifier(128, num_classes=2)  # Assuming binary classification
    
    moco_path = get_absolute_path('forapp/ckpoint/moco_covid_metadata_best_loss.pth')
    classifier_path = get_absolute_path('forapp/ckpoint/classifier_best_covid.pth')
    
    moco.load_state_dict(torch.load(moco_path, map_location=torch.device('cpu'), weights_only=True)['model_state_dict'])
    classifier.load_state_dict(torch.load(classifier_path, map_location=torch.device('cpu'), weights_only=True)['model_state_dict'])
    
    moco.eval()
    classifier.eval()
    
    return moco, classifier


# ì˜ˆì¸¡ í•¨ìˆ˜ ìˆ˜ì •
def process_audio_and_predict(audio_array, sample_rate, moco, classifier, respiratory_ailment):
    print("ì˜ˆì¸¡ í•¨ìˆ˜ ì‹œì‘")
    try:
        mfccs = librosa.feature.mfcc(y=audio_array, sr=sample_rate, n_mfcc=30)
        print(f"MFCC ìƒì„± ì™„ë£Œ. Shape: {mfccs.shape}")
        mfccs = np.expand_dims(mfccs, axis=0)
        mfccs = np.repeat(mfccs, 3, axis=0)
        
        mfccs_tensor = torch.from_numpy(mfccs).float().unsqueeze(0)
        print(f"MFCC í…ì„œ ìƒì„± ì™„ë£Œ. Shape: {mfccs_tensor.shape}")
        
        with torch.no_grad():
            features = moco.encoder_q(mfccs_tensor)
        print(f"íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ. Shape: {features.shape}")
        
        with torch.no_grad():
            outputs = classifier(features, respiratory_ailment)
            _, predicted = torch.max(outputs, 1)
        
        print(f"ì˜ˆì¸¡ ì™„ë£Œ. ê²°ê³¼: {predicted.item()}")
        return predicted.item()
    except Exception as e:
        st.error(f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None


# Streamlit ì•± ì‹œì‘
st.set_page_config(page_title="COVID-19 í˜¸í¡ìŒ ë¶„ì„", page_icon="ğŸ©º")

st.image(logo, use_column_width=True)
st.title("ğŸ•µï¸ ë‹¹ì‹ ì˜ í˜¸í¡ìŒì€ ì½”ë¡œë‚˜ë¥¼ ì•Œê³ ìˆë‹¤!")
st.write('ì•ˆë…•í•˜ì„¸ìš”, ì €í¬ëŠ” 24su deep daiv Medical AI ìŒíŒŒìŒíŒŒ íŒ€ ì…ë‹ˆë‹¤. \n ì €í¬ëŠ” **í™˜ìì˜ ë©”íƒ€ë°ì´í„°**ì™€ **í˜¸í¡ìŒ ë°ì´í„°**ë¥¼ **ëŒ€ì¡°í•™ìŠµ**ìœ¼ë¡œ ì½”ë¡œë‚˜ë¥¼ ì§„ë‹¨í•˜ëŠ” í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.')
st.write('ë‹¹ì‹ ì˜ í˜¸í¡ìŒì„ ë…¹ìŒí•˜ê³ , MFCCë¡œ ë³€í™˜í•˜ì—¬ ì½”ë¡œë‚˜ ì–‘ì„± ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•´ ë“œë¦½ë‹ˆë‹¤. ğŸ¦ ')

st.write()
st.info('ğŸ“Œ **<Demo Page ì‚¬ìš©ì„¤ëª…ì„œ>** ğŸ“Œ'
        '\n 1. ğŸ™ï¸ ì‚¬ìš©ìì˜ í˜¸í¡ìŒ ë…¹ìŒ'
        '\n     - **"START" ë²„íŠ¼**ì„ í´ë¦­í•˜ì—¬ ë…¹ìŒì„ ì‹œì‘í•©ë‹ˆë‹¤.'
        '\n     - í˜¸í¡ìŒì„ ë…¹ìŒí•œ í›„ "STOP" ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë…¹ìŒì„ ì¢…ë£Œí•©ë‹ˆë‹¤.'
        '\n     - **"ë…¹ìŒ ì¢…ë£Œ" ë²„íŠ¼**ì„ í´ë¦­í•˜ì—¬ ë…¹ìŒëœ ì˜¤ë””ì˜¤ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.'
        '\n 2. ğŸ‘©â€ğŸ’¼ ë…¹ìŒëœ í˜¸í¡ìŒ í™•ì¸ ë° ë¶„ì„'
        '\n     - ì €ì¥ëœ ì˜¤ë””ì˜¤ë¥¼ ì¬ìƒí•˜ì—¬, ì œëŒ€ë¡œ ë…¹ìŒì´ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.'
        '\n     - **"ë…¹ìŒ ë¶„ì„" ë²„íŠ¼**ì„ í´ë¦­í•˜ì—¬ MFCCë¡œ í˜¸í¡ìŒì„ ì‹œê°í™”í•©ë‹ˆë‹¤.'
        '\n 3. ğŸ©º ì½”ë¡œë‚˜ ì¦ìƒ ì„ íƒ ë° ì–‘/ìŒì„± ì˜ˆì¸¡'
        '\n     - ì½”ë¡œë‚˜ ê´€ë ¨ ì¦ìƒì´ ìˆë‹¤ë©´ ì„ íƒí•©ë‹ˆë‹¤.'
        '\n     - **"ğŸ§‘â€âš•ï¸ í˜¸í¡ìŒ ì½”ë¡œë‚˜ ìƒíƒœ ì˜ˆì¸¡" ë²„íŠ¼**ì„ í´ë¦­í•˜ì—¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.'
        '\n 4. ğŸ”„ ì´ˆê¸°í™”'
        '\n     - ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸ í›„ **"ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘" ë²„íŠ¼**ì„ í´ë¦­í•˜ì—¬ ëª¨ë“  ë°ì´í„°ë¥¼ ì´ˆê¸°í™”í•˜ê³  ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
        )
st.error('ğŸš¨ **<ì£¼ì˜ì‚¬í•­>** ğŸš¨'
         '\n - ë°ëª¨ ê²°ê³¼ëŠ” ì°¸ê³ ìš©ìœ¼ë¡œë§Œ í™œìš©í•´ì£¼ì„¸ìš”.'
         '\n - ê° ë‹¨ê³„ë¥¼ ìˆœì„œëŒ€ë¡œ ì§„í–‰í•´ ì£¼ì„¸ìš”. ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ë©´ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
)
st.markdown('---')

st.subheader("ğŸ˜¤ í˜¸í¡ìŒì„ ë…¹ìŒí•´ ì£¼ì„¸ìš”!")

# Load the model
moco, classifier = load_model()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'audio_data' not in st.session_state:
    st.session_state.audio_data = None
if 'mfccs' not in st.session_state:
    st.session_state.mfccs = None
if 'mfcc_image' not in st.session_state:
    st.session_state.mfcc_image = None
if 'prediction_result' not in st.session_state:
    st.session_state.prediction_result = None
if 'selected_symptoms' not in st.session_state:
    st.session_state.selected_symptoms = []

def reset_session_state():
    st.session_state.audio_data = None
    st.session_state.mfccs = None
    st.session_state.mfcc_image = None
    st.session_state.prediction_result = None
    st.session_state.selected_symptoms = []

webrtc_ctx = webrtc_streamer(
    key="audio-recorder",
    mode=WebRtcMode.SENDONLY,
    audio_receiver_size=1024,
    media_stream_constraints={"video": False, "audio": True},
    rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
)

if webrtc_ctx.audio_receiver:
    if webrtc_ctx.state.playing:
        st.write("ğŸ™ï¸ë…¹ìŒ ì¤‘... ë§ˆì´í¬ì— ëŒ€ê³  ë§ì”€í•´ ì£¼ì„¸ìš”!")
        
    if st.button("ë…¹ìŒ ì¢…ë£Œ"):
        audio_frames = webrtc_ctx.audio_receiver.get_frames()
        audio_data = []
        for frame in audio_frames:
            audio_data.extend(frame.to_ndarray().flatten())
        st.session_state.audio_data = np.array(audio_data, dtype=np.float32)
        st.write("ë…¹ìŒì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.session_state.mfccs = None
        st.session_state.mfcc_image = None
        st.session_state.prediction_result = None

if st.session_state.audio_data is not None:
    st.audio(st.session_state.audio_data, sample_rate=48000)
    
    if st.button("ë…¹ìŒ ë¶„ì„"):
        st.write("ğŸ‘¨â€ğŸ’¼ë…¹ìŒëœ ì˜¤ë””ì˜¤ë¥¼ ì²˜ë¦¬ ì¤‘...")
        st.markdown("---")

        sample_rate = 48000
        audio_array = st.session_state.audio_data
        data_scaled = change_volume(audio_array)

        st.subheader("ğŸ’» MFCCë¡œ í‘œí˜„ëœ ë‹¹ì‹ ì˜ í˜¸í¡ìŒì€?")
        
        try:
            with st.spinner('MFCCë¡œ í˜¸í¡ìŒì„ ë³€í™˜ ì¤‘ì— ìˆì–´ìš”...'):
                hop_length = 512
                mfccs = librosa.feature.mfcc(y=data_scaled, sr=sample_rate, n_mfcc=30)
                st.session_state.mfccs = mfccs

                plt.figure(figsize=(10, 5))
                librosa.display.specshow(mfccs, sr=sample_rate, hop_length=hop_length)
                
                buf = BytesIO()
                plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)
                buf.seek(0)
                
                st.session_state.mfcc_image = buf.getvalue()
                st.image(st.session_state.mfcc_image, caption='MFCC Spectrogram', use_column_width=True)
                plt.close()
            st.success("MFCC ë³€í™˜ ì™„ë£Œ")
        except Exception as e:
            st.error(f"MFCC ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

        wav_file = BytesIO()
        wavfile.write(wav_file, sample_rate, (audio_array * 32767).astype(np.int16))
        wav_file.seek(0)
        st.download_button(label="ğŸ“‚ í˜¸í¡ìŒ ë‹¤ìš´ë¡œë“œ", data=wav_file, file_name="respiratory_sounds.wav", mime="audio/wav")
        
    st.write()
    st.subheader("ğŸ˜· ì½”ë¡œë‚˜ ì¦ìƒ ì„ íƒ")
    st.write("ì•„ë˜ ì¦ìƒ ì¤‘ í•´ë‹¹í•˜ëŠ” ì¦ìƒì„ ì„ íƒí•´ì£¼ì„¸ìš”. í•´ë‹¹ í•˜ëŠ” ì¦ìƒì´ ì—†ë‹¤ë©´, ì„ íƒí•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.")
    # ì¦ìƒ ì„ íƒ ì„¹ì…˜ ì¶”ê°€
    symptoms = ['ê¸°ì¹¨', 'ê°ê¸°', 'ì„¤ì‚¬', 'í˜¸í¡ê³¤ë€', 'ì—´', 'ê³¼ë‹¤í”¼ë¡œ', 'ê·¼ìœ¡í†µ', 'ë¯¸ê°/í›„ê° ìƒì‹¤']
    st.session_state.selected_symptoms = st.multiselect('ë‹¤ìŒê³¼ ê°™ì€ ì½”ë¡œë‚˜ ì¦ìƒì´ ìˆì—ˆë‚˜ìš”? (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)', symptoms, st.session_state.selected_symptoms)

    if st.button("ğŸ§‘â€âš•ï¸ í˜¸í¡ìŒ ì½”ë¡œë‚˜ ìƒíƒœ ì˜ˆì¸¡"):
        if st.session_state.mfccs is not None:
            respiratory_ailment = torch.tensor([1.0]) if st.session_state.selected_symptoms else torch.tensor([0.0])
            prediction = process_audio_and_predict(st.session_state.audio_data, 48000, moco, classifier, respiratory_ailment)
            if prediction is not None:
                st.session_state.prediction_result = prediction
            else:
                st.error("ì˜ˆì¸¡ ì‹¤íŒ¨")
        else:
            st.error("ë¨¼ì € 'ë…¹ìŒ ë¶„ì„' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ MFCCë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")

    if st.session_state.prediction_result is not None:
        st.subheader("ğŸ“’ í˜¸í¡ìŒ ì½”ë¡œë‚˜ ìƒíƒœ ì˜ˆì¸¡ ê²°ê³¼")
        if st.session_state.mfcc_image:
            st.image(st.session_state.mfcc_image, caption='MFCC Spectrogram', use_column_width=True)
        if st.session_state.prediction_result == 1:
            st.warning("ì½”ë¡œë‚˜ ì–‘ì„±ìœ¼ë¡œ ì˜ˆì¸¡ë©ë‹ˆë‹¤. ì˜ë£Œì§„ê³¼ ìƒë‹´ì„ ê¶Œì¥ë“œë¦½ë‹ˆë‹¤.")
        else:
            st.success("ì½”ë¡œë‚˜ ìŒì„±ìœ¼ë¡œ ì˜ˆì¸¡ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì˜ì‹¬ ì¦ìƒì´ ìˆë‹¤ë©´ ê²€ì‚¬ë¥¼ ë°›ì•„ë³´ì„¸ìš”.")
        
        st.subheader("ì„ íƒí•œ ì¦ìƒ")
        if st.session_state.selected_symptoms:
            st.write(", ".join(st.session_state.selected_symptoms))
        else:
            st.write("ì„ íƒí•œ ì¦ìƒì´ ì—†ìŠµë‹ˆë‹¤.")

        if st.button("ğŸ”„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘"):
            reset_session_state()
            st.rerun()

else:
    st.write("ë…¹ìŒì„ ì‹œì‘í•˜ë ¤ë©´ 'START' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

st.markdown('---')
st.warning('ğŸ¤– **ë””ë²„ê·¸ ì •ë³´** (ì‹¤ì‹œê°„ í™•ì¸ìš©)'
        f'\n - ì˜¤ë””ì˜¤ ë°ì´í„° ì¡´ì¬: {st.session_state.audio_data is not None}'
        f'\n - MFCC ë°ì´í„° ì¡´ì¬: {st.session_state.mfccs is not None}'        
        f'\n - ì˜ˆì¸¡ ê²°ê³¼: {st.session_state.prediction_result}'
        f'\n - ì„ íƒí•œ ì¦ìƒ: {", ".join(st.session_state.selected_symptoms) if st.session_state.selected_symptoms else "ì—†ìŒ"}'
        )